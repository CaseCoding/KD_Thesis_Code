{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preface\n",
        "\n",
        "This code is meant to be used for finding the Hyperparameter optimization. This is not the code used for generating final results and visualization. If you want to re-create the final results, please run the 'final_version_Thesis_KDmodels.py' file located on the github repo. It already contains all of the best hyperparameters as found by myself."
      ],
      "metadata": {
        "id": "hlagmDgW-iC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loading pickle files or data"
      ],
      "metadata": {
        "id": "MD4eWpHp9imC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oXMJMSZxUGC",
        "outputId": "7614dcba-3394-43f5-cd7e-9afdbece1047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to your folder\n",
        "folder_path = '/content/drive/MyDrive/Thesis_Bsc'\n",
        "os.chdir(folder_path)\n",
        "\n",
        "# Check the current working directory to ensure you are in the correct folder\n",
        "print(\"Current Working Directory: \", os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S-JwMu1x1oi",
        "outputId": "3fe39559-451b-46f1-eee2-ecf6abbe1c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory:  /content/drive/MyDrive/Thesis_Bsc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation for Teacher and/or Student\n",
        "\n",
        "First we must setup the paramgrid with our desired hyper parameters to be tested. Run the code block containing the Functions and train/test/validation function before running the Param_grid code block. This will test for the Teacher and Student, as these have the most impact on the following methods ensure this is done with the most care."
      ],
      "metadata": {
        "id": "sDX0KaAj9njv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Define the hyperparameter grid to a selected amount of Hyper parameters you want to test.\n",
        "param_grid = {\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'batch_size': [64, 128, 256],\n",
        "    'num_epochs': [10, 20, 30]\n",
        "}\n",
        "\n",
        "# Create a list of all combinations of hyperparameters\n",
        "param_combinations = list(itertools.product(param_grid['learning_rate'], param_grid['batch_size'], param_grid['num_epochs']))\n",
        "\n",
        "best_val_accuracy = 0\n",
        "best_params = None\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Iterate over all combinations of hyperparameters\n",
        "for learning_rate, batch_size, num_epochs in param_combinations:\n",
        "    # Create data loaders with the current batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = LightNN()  # or DeepNN() depending on which model you want to optimize\n",
        "\n",
        "    # Train the model\n",
        "    train(model, train_loader, val_loader, epochs=num_epochs, learning_rate=learning_rate, device=device)\n",
        "\n",
        "    # Validate the model\n",
        "    val_loss, val_accuracy = validate(model, val_loader, device)\n",
        "\n",
        "    # Track the best performing hyperparameters\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_params = {'learning_rate': learning_rate, 'batch_size': batch_size, 'num_epochs': num_epochs}\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.2f}%\")\n",
        "\n",
        "# Train the final model with the best hyperparameters on the combined training and validation set\n",
        "full_train_loader = DataLoader(full_train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=2)\n",
        "final_model = LightNN()  # or DeepNN()\n",
        "train(final_model, full_train_loader, val_loader, epochs=best_params['num_epochs'], learning_rate=best_params['learning_rate'], device=device)\n",
        "\n",
        "# Test the final model\n",
        "test_accuracy = test(final_model, test_loader, device)\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CIKpCMfCv4UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sZF3sHgv3Rl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "# from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "# Check if GPU is available, and if not, use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Function to load a pickle file\n",
        "def load_pickle(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "    return data\n",
        "\n",
        "# Replace 'file_path.pkl' with the actual path to your pickled files\n",
        "image_data_path = '/content/drive/MyDrive/Thesis_Bsc/image_data.pkl'\n",
        "one_hot_encoded_data_path = '/content/drive/MyDrive/Thesis_Bsc/one_hot_encoded_data.pkl'\n",
        "test_image_data_path = '/content/drive/MyDrive/Thesis_Bsc/test_image_data.pkl'\n",
        "test_one_hot_encoded_data_path = '/content/drive/MyDrive/Thesis_Bsc/test_one_hot_encoded_data.pkl'\n",
        "\n",
        "# Unpickling the data\n",
        "image_data = load_pickle(image_data_path)\n",
        "one_hot_encoded_data = load_pickle(one_hot_encoded_data_path)\n",
        "test_image_data = load_pickle(test_image_data_path)\n",
        "test_one_hot_encoded_data = load_pickle(test_one_hot_encoded_data_path)\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "calculated_mean = torch.tensor([0.5075, 0.5064, 0.5082]).mean().item()\n",
        "calculated_std = torch.tensor([0.2556, 0.2558, 0.2541]).mean().item()\n",
        "\n",
        "\n",
        "transforms_data = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[calculated_mean], std=[calculated_std])\n",
        "])\n",
        "\n",
        "# Assuming `train_images` and `train_labels` are your pre-loaded training data and labels\n",
        "# And `test_images` and `test_labels` are your pre-loaded test data and labels\n",
        "train_dataset = SimpleDataset(image_data, one_hot_encoded_data, transform=transforms_data)\n",
        "test_dataset = SimpleDataset(test_image_data, test_one_hot_encoded_data, transform=transforms_data)\n",
        "\n",
        "# Define the split ratio for train and validation sets\n",
        "train_size = int(0.85 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            labels = labels.squeeze(1).max(dim=1)[1]\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    return avg_val_loss, val_accuracy\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs, learning_rate, device):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            labels = labels.squeeze(1).max(dim=1)[1]\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_val_loss, val_accuracy = validate(model, val_loader, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize metrics\n",
        "    accuracy_metric = Accuracy(num_classes=6, task='multiclass').to(device)\n",
        "    precision_metric = Precision(num_classes=6, task='multiclass').to(device)\n",
        "    recall_metric = Recall(num_classes=6, task='multiclass').to(device)\n",
        "    f1_metric = F1Score(num_classes=6, task='multiclass').to(device)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Loop through batches in the test set\n",
        "    with torch.no_grad():\n",
        "        for batch_index, (inputs, labels) in enumerate(test_loader):\n",
        "            # Convert labels from one-hot encoded to class indices\n",
        "            labels = labels.squeeze(1).max(dim=1)[1]\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Update metrics\n",
        "            accuracy_metric.update(predicted, labels)\n",
        "            precision_metric.update(predicted, labels)\n",
        "            recall_metric.update(predicted, labels)\n",
        "            f1_metric.update(predicted, labels)\n",
        "            print(\"Predictions:\", predicted)\n",
        "            print(\"True labels:\", labels)\n",
        "\n",
        "            all_preds.append(predicted.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    # Compute overall metrics after going through all batches\n",
        "    overall_accuracy = accuracy_metric.compute()\n",
        "    overall_precision = precision_metric.compute()\n",
        "    overall_recall = recall_metric.compute()\n",
        "    overall_f1_score = f1_metric.compute()\n",
        "\n",
        "    # Print overall metrics\n",
        "    print(f'Overall Test - '\n",
        "          f'Accuracy: {overall_accuracy:.2f}, '\n",
        "          f'Precision: {overall_precision:.3f}, '\n",
        "          f'Recall: {overall_recall:.3f}, '\n",
        "          f'F1: {overall_f1_score:.3f}')\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    cm_df = pd.DataFrame(cm)\n",
        "\n",
        "    model_class_name = model.__class__.__name__\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    csv_file_path = f'./results/confusion_matrix_{model_class_name}_{timestamp}.csv'\n",
        "    cm_df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "    # Return a dictionary of the overall metrics\n",
        "    return {\n",
        "        \"accuracy\": overall_accuracy.item(),\n",
        "        \"precision\": overall_precision.item(),\n",
        "        \"recall\": overall_recall.item(),\n",
        "        \"f1_score\": overall_f1_score.item()\n",
        "    }\n",
        "\n",
        "# Deeper neural network class to be used as teacher:\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4608, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Lightweight neural network class to be used as student:\n",
        "class LightNN(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super(LightNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2304, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(42)\n",
        "nn_deep = DeepNN(num_classes=6).to(device)\n",
        "#train(nn_deep, train_loader, val_loader, epochs=25, learning_rate=0.001, device=device)\n",
        "#results_deep = test(nn_deep, test_loader, device)\n",
        "\n",
        "# Instantiate the lightweight network:\n",
        "torch.manual_seed(42)\n",
        "nn_light = LightNN(num_classes=6).to(device)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "new_nn_light = LightNN(num_classes=6).to(device)\n",
        "\n",
        "# Print the norm of the first layer of the initial lightweight model\n",
        "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
        "# Print the norm of the first layer of the new lightweight model\n",
        "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())\n",
        "\n",
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
        "print(f\"DeepNN parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")\n",
        "\n",
        "#train(nn_light, train_loader, val_loader, epochs=10, learning_rate=0.001, device=device)\n",
        "#results_light_ce = test(nn_light, test_loader, device)\n",
        "\n",
        "# Print all metrics for the deep model\n",
        "print(\"Deep Model Metrics:\")\n",
        "for metric, value in results_deep.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.2f}%\")\n",
        "\n",
        "# Print all metrics for the lightweight model\n",
        "print(\"Lightweight Model Metrics:\")\n",
        "for metric, value in results_light_ce.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation for KD approach using output labels\n",
        "Same as previous but modified for the first distillation function"
      ],
      "metadata": {
        "id": "wzYeRm6P_NQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_knowledge_distillation(teacher, student, train_loader, val_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train()  # Student to train mode\n",
        "\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        student.train()  # Ensure the student model is in training mode\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            # Remove the middle dimension and convert one-hot encoded labels to class indices\n",
        "            labels = labels.squeeze(1).max(dim=1)[1]\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits = student(inputs)\n",
        "\n",
        "            # Soften the student logits by applying softmax first and log() second\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
        "\n",
        "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
        "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_val_loss, val_accuracy = validate(student, val_loader, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model_state = student.state_dict().copy()\n",
        "\n",
        "    student.load_state_dict(best_model_state)\n",
        "    return best_val_accuracy\n"
      ],
      "metadata": {
        "id": "E6ifUwiPBZQh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            labels = labels.squeeze(1).max(dim=1)[1]\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    return avg_val_loss, val_accuracy\n"
      ],
      "metadata": {
        "id": "k06irFMkDw9Q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'T': [1.5, 2, 2.5],\n",
        "    'soft_target_loss_weight': [0.25, 0.35, 0.45],\n",
        "    'ce_loss_weight': [0.75, 0.65, 0.55]\n",
        "}\n",
        "\n",
        "# Create a list of all combinations of hyperparameters\n",
        "param_combinations = list(itertools.product(param_grid['T'], param_grid['soft_target_loss_weight'], param_grid['ce_loss_weight']))\n",
        "\n",
        "nn_deep = DeepNN(num_classes=6).to(device)\n",
        "nn_deep.eval()  # Set the teacher model to evaluation mode\n",
        "\n",
        "# Best hyperparameters initialization\n",
        "best_val_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "# Loop through all combinations of hyperparameters\n",
        "for T, soft_target_loss_weight, ce_loss_weight in param_combinations:\n",
        "    print(f\"Training with T={T}, soft_target_loss_weight={soft_target_loss_weight}, ce_loss_weight={ce_loss_weight}\")\n",
        "\n",
        "    # Ensure complementary loss weights\n",
        "    if soft_target_loss_weight + ce_loss_weight != 1.0:\n",
        "        print(f\"Skipping invalid combination: T={T}, soft_target_loss_weight={soft_target_loss_weight}, ce_loss_weight={ce_loss_weight}\")\n",
        "        continue\n",
        "\n",
        "    # Initialize student model\n",
        "    torch.manual_seed(42)\n",
        "    student_model = LightNN(num_classes=6).to(device)\n",
        "\n",
        "    # Train and validate the student model\n",
        "    val_accuracy = train_knowledge_distillation(\n",
        "        teacher=nn_deep,\n",
        "        student=student_model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        epochs=25,\n",
        "        learning_rate=0.001,\n",
        "        T=T,\n",
        "        soft_target_loss_weight=soft_target_loss_weight,\n",
        "        ce_loss_weight=ce_loss_weight,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Track the best performing hyperparameters based on validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_params = {\n",
        "            'T': T,\n",
        "            'soft_target_loss_weight': soft_target_loss_weight,\n",
        "            'ce_loss_weight': ce_loss_weight\n",
        "        }\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "0gkLPUO-Dyg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we need to test for the other 2 methods"
      ],
      "metadata": {
        "id": "RylKdkHmEWFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedDeepNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super(ModifiedDeepNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),  # Adjusted for 1 input channel\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # Assuming an input image size of 48x48 for correct flattened size calculation\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4608, 512),  # Adjusted linear layer size\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        flattened_conv_output = torch.flatten(x, 1)\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)\n",
        "        return x, flattened_conv_output_after_pooling\n",
        "\n",
        "class ModifiedLightNNCosine(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super(ModifiedLightNNCosine, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # Assuming grayscale images\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # Adjusted linear layer size to match 1024\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2304, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        flattened_conv_output = torch.flatten(x, 1)\n",
        "        x = self.classifier(flattened_conv_output)\n",
        "        return x, flattened_conv_output\n",
        "\n",
        "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
        "modified_nn_deep = ModifiedDeepNNCosine(num_classes=6).to(device)\n",
        "modified_nn_deep.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Once again ensure the norm of the first layer is the same for both networks\n",
        "print(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())\n",
        "print(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())\n",
        "\n",
        "# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\n",
        "torch.manual_seed(42)\n",
        "modified_nn_light = ModifiedLightNNCosine(num_classes=6).to(device)\n",
        "print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())\n",
        "\n",
        "# Adjust sample input for grayscale images\n",
        "sample_input = torch.randn(128, 1, 48,48).to(device) # Batch size: 128, Filters: 1 (grayscale), Image size: 32x32\n",
        "\n",
        "# Pass the input through the student\n",
        "logits, hidden_representation = modified_nn_light(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n",
        "print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
        "\n",
        "# Pass the input through the teacher\n",
        "logits, hidden_representation = modified_nn_deep(sample_input)\n",
        "\n",
        "# Print the shapes of the tensors\n",
        "print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n",
        "print(\"Teacher hidden representation shape:\", hidden_representation.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "OUvyltJZEZUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cosine_loss(teacher, student, train_loader, val_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    cosine_loss = nn.CosineEmbeddingLoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train()  # Student to train mode\n",
        "\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        student.train()  # Ensure the student model is in training mode\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            labels = labels.squeeze(1).max(dim=1)[1]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with the teacher model and keep only the hidden representation\n",
        "            with torch.no_grad():\n",
        "                _, teacher_hidden_representation = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits, student_hidden_representation = student(inputs)\n",
        "\n",
        "            # Calculate the cosine loss. Target is a vector of ones.\n",
        "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n",
        "\n",
        "            # Calculate the true label loss\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_val_loss, val_accuracy = validate(student, val_loader, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model_state = student.state_dict().copy()\n",
        "\n",
        "    student.load_state_dict(best_model_state)\n",
        "    return best_val_accuracy\n"
      ],
      "metadata": {
        "id": "LI5aQcWdFIpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            labels = labels.squeeze(1).max(dim=1)[1]\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    return avg_val_loss, val_accuracy\n"
      ],
      "metadata": {
        "id": "CkmZiWghFRI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'hidden_rep_loss_weight': [0.25, 0.35, 0.45],\n",
        "    'ce_loss_weight': [0.75, 0.65, 0.55]\n",
        "}\n",
        "\n",
        "# Create a list of all combinations of hyperparameters\n",
        "param_combinations = list(itertools.product(param_grid['hidden_rep_loss_weight'], param_grid['ce_loss_weight']))\n",
        "\n",
        "# Ensure complementary loss weights\n",
        "param_combinations = [(hrw, clw) for hrw, clw in param_combinations if hrw + clw == 1.0]\n",
        "\n",
        "# Load the pretrained teacher model\n",
        "teacher_model_path = 'path_to_saved_teacher_model.pth'\n",
        "nn_deep = DeepNN(num_classes=6).to(device)\n",
        "nn_deep.load_state_dict(torch.load(teacher_model_path))\n",
        "nn_deep.eval()  # Set the teacher model to evaluation mode\n",
        "\n",
        "# Load the modified teacher model weights\n",
        "modified_nn_deep = ModifiedDeepNNCosine(num_classes=6).to(device)\n",
        "modified_nn_deep.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Best hyperparameters initialization\n",
        "best_val_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "# Loop through all combinations of hyperparameters\n",
        "for hidden_rep_loss_weight, ce_loss_weight in param_combinations:\n",
        "    print(f\"Training with hidden_rep_loss_weight={hidden_rep_loss_weight}, ce_loss_weight={ce_loss_weight}\")\n",
        "\n",
        "    # Initialize student model\n",
        "    torch.manual_seed(42)\n",
        "    student_model = ModifiedLightNNCosine(num_classes=6).to(device)\n",
        "\n",
        "    # Train and validate the student model\n",
        "    val_accuracy = train_cosine_loss(\n",
        "        teacher=modified_nn_deep,\n",
        "        student=student_model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        epochs=25,\n",
        "        learning_rate=0.001,\n",
        "        hidden_rep_loss_weight=hidden_rep_loss_weight,\n",
        "        ce_loss_weight=ce_loss_weight,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Track the best performing hyperparameters based on validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_params = {\n",
        "            'hidden_rep_loss_weight': hidden_rep_loss_weight,\n",
        "            'ce_loss_weight': ce_loss_weight\n",
        "        }\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Bo6ts08ZFSHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mse regression\n"
      ],
      "metadata": {
        "id": "eFNuMTrMF5jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the sample input only from the convolutional feature extractor\n",
        "convolutional_fe_output_student = nn_light.features(sample_input)\n",
        "convolutional_fe_output_teacher = nn_deep.features(sample_input)\n",
        "\n",
        "# Print their shapes\n",
        "print(\"Student's feature extractor output shape: \", convolutional_fe_output_student.shape)\n",
        "print(\"Teacher's feature extractor output shape: \", convolutional_fe_output_teacher.shape)\n",
        "\n",
        "class ModifiedDeepNNRegressor(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super(ModifiedDeepNNRegressor, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),  # Adjusted for 1 input channel\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # Assuming an input image size of 48x48\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(4608, 512),  # Corrected to match output from the feature extractor\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        conv_feature_map = x\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, conv_feature_map\n",
        "\n",
        "class ModifiedLightNNRegressor(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super(ModifiedLightNNRegressor, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # Adjusted for 1 input channel\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        )        # Assuming an input image size of 48x48\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2304, 256),  # Adjusted to match output from the feature extractor\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        regressor_output = self.regressor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, regressor_output\n",
        "\n",
        "# Example of initializing and preparing the models\n",
        "modified_deep_regressor = ModifiedDeepNNRegressor(num_classes=6).to(device)\n",
        "modified_light_regressor = ModifiedLightNNRegressor(num_classes=6).to(device)"
      ],
      "metadata": {
        "id": "dkJzk0J7F7SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mse_loss(teacher, student, train_loader, val_loader, epochs, learning_rate, feature_map_weight, ce_loss_weight, device):\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    mse_loss = nn.MSELoss()\n",
        "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
        "\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    teacher.eval()  # Teacher set to evaluation mode\n",
        "    student.train()  # Student to train mode\n",
        "\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        student.train()  # Ensure the student model is in training mode\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            labels = labels.squeeze(1).max(dim=1)[1]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Ignore teacher logits, get only the feature map\n",
        "            with torch.no_grad():\n",
        "                _, teacher_feature_map = teacher(inputs)\n",
        "\n",
        "            # Forward pass with the student model\n",
        "            student_logits, student_feature_map = student(inputs)\n",
        "\n",
        "            # Calculate the MSE loss for the feature maps\n",
        "            hidden_rep_loss = mse_loss(student_feature_map, teacher_feature_map)\n",
        "\n",
        "            # Calculate the Cross-Entropy loss for the actual labels\n",
        "            label_loss = ce_loss(student_logits, labels)\n",
        "\n",
        "            # Weighted sum of the two losses\n",
        "            loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_val_loss, val_accuracy = validate(student, val_loader, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model_state = student.state_dict().copy()\n",
        "\n",
        "    student.load_state_dict(best_model_state)\n",
        "    return best_val_accuracy\n"
      ],
      "metadata": {
        "id": "LYuhjfonGZq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            labels = labels.squeeze(1).max(dim=1)[1]\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    return avg_val_loss, val_accuracy\n"
      ],
      "metadata": {
        "id": "7nzAQfz4Gc8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'feature_map_weight': [0.25, 0.35, 0.45],\n",
        "    'ce_loss_weight': [0.75, 0.65, 0.55]\n",
        "}\n",
        "\n",
        "# Create a list of all combinations of hyperparameters\n",
        "param_combinations = list(itertools.product(param_grid['feature_map_weight'], param_grid['ce_loss_weight']))\n",
        "\n",
        "# Ensure complementary loss weights\n",
        "param_combinations = [(fmw, clw) for fmw, clw in param_combinations if fmw + clw == 1.0]\n",
        "\n",
        "# Load the pretrained teacher model\n",
        "teacher_model_path = 'path_to_saved_teacher_model.pth'\n",
        "nn_deep = DeepNN(num_classes=6).to(device)\n",
        "nn_deep.load_state_dict(torch.load(teacher_model_path))\n",
        "nn_deep.eval()  # Set the teacher model to evaluation mode\n",
        "\n",
        "# Load the modified teacher model weights\n",
        "modified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=6).to(device)\n",
        "modified_nn_deep_reg.load_state_dict(nn_deep.state_dict())\n",
        "\n",
        "# Best hyperparameters initialization\n",
        "best_val_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "# Loop through all combinations of hyperparameters\n",
        "for feature_map_weight, ce_loss_weight in param_combinations:\n",
        "    print(f\"Training with feature_map_weight={feature_map_weight}, ce_loss_weight={ce_loss_weight}\")\n",
        "\n",
        "    # Initialize student model\n",
        "    torch.manual_seed(42)\n",
        "    student_model = ModifiedLightNNRegressor(num_classes=6).to(device)\n",
        "\n",
        "    # Train and validate the student model\n",
        "    val_accuracy = train_mse_loss(\n",
        "        teacher=modified_nn_deep_reg,\n",
        "        student=student_model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        epochs=25,\n",
        "        learning_rate=0.001,\n",
        "        feature_map_weight=feature_map_weight,\n",
        "        ce_loss_weight=ce_loss_weight,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Track the best performing hyperparameters based on validation accuracy\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_params = {\n",
        "            'feature_map_weight': feature_map_weight,\n",
        "            'ce_loss_weight': ce_loss_weight\n",
        "        }\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "4EEsen6BGdo1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}